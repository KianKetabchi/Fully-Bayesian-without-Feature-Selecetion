# -*- coding: utf-8 -*-
"""Full_bayes__bestresult_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FwZQ0RZ-gvAV4OO24ZXxsfv6Cw5vnhOk
"""

import numpy as np

import pandas as pd

import math

import scipy

from scipy.special import gammaln

from scipy.special import gamma

from scipy.stats import dirichlet

from numpy import asarray

import sys

from sklearn import datasets

from sklearn import preprocessing

from sklearn.cluster import KMeans

from numpy import min as MIN, max as MAX

from numpy import mean as MEAN

from numpy import var as VAR

import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix 

from sklearn.metrics import accuracy_score 

from sklearn.metrics import classification_report 

from sklearn.metrics import accuracy_score

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

from sklearn.metrics import f1_score

from sklearn.metrics import precision_recall_fscore_support

import pylab as pl

from scipy.stats import dirichlet

dataset = pd.read_csv('breast_tissue_editted - Sheet1.csv')


data_x = dataset.iloc[:, 0:dataset.shape[1]-1]

y = dataset.iloc[:,-1].values
y

num_sample = data_x.shape[0]
num_sample
#num_dimension = data_x.shape[1]
#num_dimension

num_dimension = data_x.shape[1]
num_dimension

#normalizing data

min_max_scaler = preprocessing.MinMaxScaler()

data_normalize = min_max_scaler.fit_transform(data_x)

for i in range(num_sample):

    for j in range(num_dimension):

        if data_normalize[i,j] ==0: 

            data_normalize[i,j]= 0.00001

        elif data_normalize[i,j]==1:

            data_normalize[i,j]= 0.9999

data_normalize

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 2)
kmeans.fit(data_normalize)
class_labels = kmeans.labels_

confusion_matrix(class_labels,y)

def split_pixels_based_on_label(labels, data):

    clusters_obj = {}

    for index, label in enumerate(labels):

        if not (label in clusters_obj):

            clusters_obj[label] = []

        clusters_obj[label].append(data[index])

    return clusters_obj





clusters_set= split_pixels_based_on_label(class_labels, data_normalize)

clusters_set

#Determine LN Parameters
num_cluster= 2

a =  np.array([1., 4., 1., 1., 9., 4., 3., 5., 9., 5., 5., 4., 3., 6., 8., 2.,
        1., 4., 6., 3., 6., 8., 2., 2., 4., 6., 5., 4., 1., 2., 5., 6.,
        1., 8., 1., 6., 2., 5., 5., 8., 5., 7., 5., 2., 9., 4., 8., 6.,
        9., 6., 6., 8., 2., 2., 5., 4., 5., 3., 3., 2., 6., 4., 6., 5.,
        3., 5., 5., 1., 2., 4., 6., 8., 7., 5., 9., 5., 2., 6., 3., 7.,
        4., 7., 7., 1., 4., 7., 3., 1., 5., 7., 6., 6., 8., 5., 5., 9.,
        2., 5., 3., 3., 2., 2., 2., 9., 7., 3., 8., 2., 9., 1., 3., 6.,
        3., 2., 4., 7., 2., 7., 7., 7., 7., 8., 5., 5., 2., 4., 8., 5.,
        8., 9., 8., 5., 5., 3., 5., 5., 6., 7., 3., 4., 8., 5., 9., 4.,
        4., 7., 9., 9., 8., 1., 8., 1., 6., 2., 9., 2., 5., 5., 4., 6.,
        3., 9., 2., 6., 5., 7., 9., 8., 3., 2., 6., 3., 3., 2., 1., 6.,
        5., 1., 2., 1., 8., 5., 1., 4., 3., 8., 9., 8., 3., 6., 8., 4.,
        4., 5., 1., 2., 1., 6., 1., 9., 9., 7., 6., 7., 9., 2., 7., 4.,
        7., 9., 3., 2., 1., 9., 8., 3., 5., 6., 4., 5., 1., 7., 8., 2.,
        3., 6., 3., 1., 3., 4., 3., 5., 8., 3., 9., 3., 5., 2., 4., 7.,
        5., 8., 4., 2., 3., 5., 2., 5., 3., 3., 9., 7.]).reshape(num_cluster,num_dimension)

b = np.array([1., 5., 3., 4., 6., 7., 4., 8., 8., 3., 9., 3., 2., 3., 8., 1.,
        8., 6., 3., 2., 5., 4., 6., 4., 4., 4., 1., 3., 6., 9., 2., 5.,
        3., 8., 4., 2., 7., 9., 4., 1., 4., 2., 3., 5., 2., 8., 9., 5.,
        9., 6., 4., 1., 2., 4., 6., 4., 7., 7., 5., 9., 8., 3., 8., 1.,
        6., 5., 9., 9., 9., 8., 8., 5., 8., 8., 9., 4., 9., 4., 1., 9.,
        8., 2., 4., 8., 2., 9., 6., 4., 6., 8., 6., 7., 4., 3., 7., 6.,
        4., 1., 4., 1., 4., 9., 1., 8., 9., 5., 9., 4., 2., 8., 7., 3.,
        5., 6., 1., 5., 8., 9., 6., 1., 4., 9., 9., 9., 8., 4., 2., 7.,
        1., 4., 8., 3., 1., 8., 1., 8., 6., 9., 9., 9., 8., 3., 4., 1.,
        6., 5., 1., 3., 7., 5., 9., 8., 2., 3., 2., 5., 4., 1., 1., 2.,
        2., 1., 8., 5., 6., 8., 2., 1., 7., 7., 7., 3., 5., 3., 7., 8.,
        4., 9., 1., 8., 8., 1., 1., 7., 7., 9., 2., 3., 9., 2., 4., 9.,
        4., 4., 4., 1., 6., 1., 8., 1., 4., 5., 4., 6., 6., 4., 5., 8.,
        1., 3., 2., 5., 6., 7., 3., 6., 6., 4., 4., 5., 4., 3., 1., 1.,
        8., 6., 8., 4., 6., 9., 9., 3., 1., 5., 1., 1., 2., 9., 8., 9.,
        7., 1., 8., 9., 1., 7., 8., 9., 7., 5., 9., 5.]).reshape(num_cluster,num_dimension)

c = np.ones([num_cluster,num_dimension])

c.shape

theta_matrix_initial = np.array([a,b,c]).reshape(3,num_dimension*2)
theta_matrix_initial

def mixer_estimator(cluster_set, p_size):

    return [len(cluster_set[cluster])/p_size for cluster in cluster_set]



mix = mixer_estimator(clusters_set, num_sample)

mix

# pdf multiply with weight  for finding z or responsiblity 

def posterior(a,b,c,data_normalize, mix, num_cluster,num_sample):
  second_part_one = np.matmul(np.log(data_normalize),(a-1).T)
  one_minus_data = np.subtract(1 , data_normalize) + 0.001
  third_part_one = np.matmul(np.log(one_minus_data),(b-1).T)   
  fifth_part = 1
  pdf = second_part_one+third_part_one+fifth_part
  posteriori = (pdf*mix). reshape(num_sample , num_cluster)
  zhat = posteriori/np.asarray(np.sum(posteriori, axis=1)).reshape(num_sample, 1)

  return zhat

zhat = posterior(a,b,c,data_normalize, mix, num_cluster,num_sample)
zhat

#exp(log)
def zed(zhat):
    z = np.zeros(zhat.shape)
    z[np.arange(len(zhat)), zhat.argmax(1)] = 1
    return (z)

z_matrix = zed(zhat)
z_matrix.shape

df = pd.DataFrame(zhat)
df.plot.hist()
plt.show()

def z_creator (z_matrix):

    z_label = np.zeros((num_sample,1))

    for i in range(num_sample):

        for j in range(num_cluster):

            if z_matrix[i,j]== 1:

                z_label[i] = j

    return z_label

    

z_label = z_creator(z_matrix)
z_label

z_matrix

def nt(z):
  nt = np.zeros(z.shape)
  nt = np.sum(z,axis = 0)
  return(nt)

n_counter = nt(z_matrix)
n_counter

# posterior distribution for weight (Equation)
eta = ([1/num_dimension]*num_cluster)
def probability(eta,n_counter):
  p = np.random.dirichlet(eta+n_counter)
  return p

#def transition_model(num_cluster, theta_matrix_initial):

    #sigma = np.ones((theta_matrix_initial.shape[1])) * 0.15

    #return np.random.normal(theta_matrix_initial,list(sigma))

#tr = transition_model(num_cluster, theta_matrix_initial)
#tr

def log_likelihood(a,b,c,data_normalize, num_cluster,num_sample):
  second_part_one = np.matmul(np.log(data_normalize),(a-1).T)
  one_minus_data = np.subtract(1 , data_normalize) + 0.001
  third_part_one = np.matmul(np.log(one_minus_data),(b-1).T)   
  fifth_part = 1
  pdf = second_part_one+third_part_one+fifth_part
  return pdf

log_like_initial = log_likelihood(a,b,c,data_normalize, num_cluster,num_sample)
log_like_initial

#hyper_prameter Matrix
u = np.random.randint(1,10,(num_cluster,num_dimension))
v = np.random.randint(1,10,(num_cluster,num_dimension))
r = np.random.randint(1,10,(num_cluster,num_dimension))
s = np.random.randint(1,10,(num_cluster,num_dimension))
f = np.random.randint(1,10,(num_cluster,num_dimension))
g = np.random.randint(1,10,(num_cluster,num_dimension))

# check the sum 
def log_prior1(a,b,c):
  log_p_c = np.absolute((f*np.log(g))+((f-1)*np.log(c))-(g*c)-(gammaln(f)))
  log_p_a = np.absolute((u*np.log(v))+((u-1)*np.log(a))-(v*a)-(gammaln(u)))
  log_p_b = np.absolute((r*np.log(s))+((r-1)*np.log(b))-(s*b)-(gammaln(r)))
  return log_p_a,log_p_b,log_p_c

a_new= np.random.lognormal(a,0.5,(num_cluster,num_dimension))
a_new = np.array(a_new)
b_new = np.random.lognormal(b,0.55,(num_cluster,num_dimension))
b_new = np.array(b_new)
c_new = np.random.lognormal(c,0.45,(num_cluster,num_dimension))
c_new = np.array(c_new)


theta_matrix_new = np.array([a_new,b_new,c_new]).reshape(3,num_dimension*2)
theta_matrix_new
#theta_matrix_initial

log_prior_new = log_prior1(a_new,b_new,c_new)
log_prior_new

log_prior_old = log_prior1(a,b,c)
log_prior_old

log_like_initial = log_likelihood(a,b,c,data_normalize, num_cluster,num_sample)
log_like_initial

log_like_new = log_likelihood(a_new,b_new,c_new,data_normalize, num_cluster,num_sample)
log_like_new = np.absolute(log_like_new)
log_like_new

theta_matrix_initial.shape

#theta
theta_initial= np.sum(theta_matrix_initial,axis = 1)
theta_initial = np.sum(theta_initial,axis = 0)
theta_initial

theta_new = np.sum(theta_matrix_new,axis =1)
theta_new = np.sum(theta_new,axis = 0)
theta_new

#likelihood 
like_new = np.sum(log_like_new, axis = 1)
like_new = np.sum(like_new, axis = 0)

like_old = np.sum(log_like_initial,axis =1)
like_old = np.sum(like_old, axis = 0)

prior_new = np.sum(log_prior_new,axis = 1)
prior_new = np.sum(prior_new,axis = 0)
prior_new = np.sum(prior_new,axis = 0)
prior_new

prior_old = np.sum(log_prior_old,axis = 1)
prior_old = np.sum(prior_old, axis = 0)
prior_old  = np.sum(prior_old,axis = 0)
prior_old

def y_predicted(z,num_sample):

    dfObj = pd.DataFrame(z_matrix, columns=list("01"))

    maxValueIndexObj = []

    # get the column name of max values in every row

    maxValueIndexObj.append(dfObj.idxmax(axis=1))

    y=[]



    maxValueIndexObj =asarray(maxValueIndexObj).reshape(num_sample, 1)

    y.append([int(i) for i in maxValueIndexObj])

    



    # print(maxValueIndexObj)

    # print("y is:", asarray(y))

    return asarray(y).T




y_predict = y_predicted(z_matrix,num_sample)
y_predict
results = confusion_matrix(y, y_predict)
print ('Confusion Matrix :')
print(results)
score=accuracy_score(y, y_predict)
print ('Accuracy Score :')
print(score)
print ('Report : ')
print(classification_report(y, y_predict))
accuracy = accuracy_score(y, y_predict)

print(a,b)


#print ('Confusion Matrix :')

#print(results) 

#score=accuracy_score(y, y_predict) 

#print ('Accuracy Score :')

#print(score)

#print ('Report : ')

#print(classification_report(y, y_predict))

#accuracy = accuracy_score(y, y_predict)

#print(a_,b_,c_)
#accuracy

def metropolis_hastings(theta_matrix_initial,theta_matrix_new,iterations):
  threshold = np.random.uniform(0, 1)
  accepted = []
  rejected = []

  x = theta_matrix_initial


  for i in range(iterations):
    x_new = theta_matrix_new
    r_nominator = theta_initial+(like_new*prior_new)
    r_denominator = theta_new+(like_old*prior_old)
    ratio = np.absolute(np.divide(r_nominator, r_denominator.T))
    r = np.exp(ratio)
    if r<threshold:
      x = x_new
      print(accepted.append(x_new))
    else:
      print(rejected.append(x_new))
        
      return np.array(accepted).shape, np.array(rejected)

mh = metropolis_hastings(theta_matrix_initial,theta_matrix_new,1000)
mh